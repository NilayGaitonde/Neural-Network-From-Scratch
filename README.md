# Neural Network from Scratch
This project implements a basic neural network using only NumPy and Pandas. It demonstrates core concepts like forward propagation, backpropagation, and gradient descent for training.

# Key Features
- Fully connected network layers implemented in NumPy
- Tanh and softmax activation functions
- Forward and backpropagation passes
- Gradient descent optimization to update weights
- Trained and tested on MNIST dataset
# Usage
The code is contained in a Jupyter notebook for easy execution and readability. To use:

1. Download the notebook file and mnist dataset
2. Ensure NumPy and Pandas are installed
3. Run the cells in the notebook to train the network on the MNIST data
4. The notebook walks through each step with explanations and visualizations. Easily tweak the network configuration and training parameters to experiment with different architectures.

The trained model achieves 90.75% accuracy on the test set.

Links
View and run the Jupyter notebook on Kaggle: https://www.kaggle.com/code/nilaygaitonde/mnist-from-scratch?scriptVersionId=157118372
